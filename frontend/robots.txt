# MyKitchen Recipe Finder - Robots.txt
# This file tells search engine crawlers which pages they can and cannot access

User-agent: *
Allow: /

# Allow access to all pages by default
Allow: /recipes/
Allow: /search
Allow: /about
Allow: /contact

# Disallow access to admin or private areas (if any exist in future)
Disallow: /admin/
Disallow: /api/
Disallow: /backend/
Disallow: /private/
Disallow: /*.json$
Disallow: /firebase-config/

# Allow access to important static files
Allow: /sitemap.xml
Allow: /favicon.ico
Allow: /*.css
Allow: /*.js
Allow: /*.png
Allow: /*.jpg
Allow: /*.jpeg
Allow: /*.gif
Allow: /*.webp
Allow: /*.svg

# Disallow access to specific file types that shouldn't be indexed
Disallow: /*.txt$
Disallow: /*.log$
Disallow: /*.env$

# Crawl delay to be respectful to server resources
Crawl-delay: 1

# Sitemap location
Sitemap: https://yourusername.github.io/recipe-finder/sitemap.xml

# Additional sitemap locations for different content types
# Sitemap: https://yourusername.github.io/recipe-finder/sitemap-recipes.xml
# Sitemap: https://yourusername.github.io/recipe-finder/sitemap-images.xml

# Special instructions for specific bots
User-agent: Googlebot
Allow: /
Crawl-delay: 1

User-agent: Bingbot
Allow: /
Crawl-delay: 1

User-agent: facebookexternalhit
Allow: /

User-agent: Twitterbot
Allow: /

# Block bad bots and scrapers
User-agent: AhrefsBot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /